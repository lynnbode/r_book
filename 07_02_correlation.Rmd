## Korrelationen

```{r, echo = FALSE, warning=FALSE, message=FALSE, error=FALSE}
pacman::p_load(tidyverse, sjmisc)
# Einlesen der Daten
df <- haven::read_sav("data/ZA6738_v1-0-0_generation_z_recoded.sav") %>% 
  row_means(einstellung_politiker_verstaendlich,
            einstellung_entscheidungsprozess_undurchsichtig,
            einstellung_keine_ueberzeugende_partei,
            einstellung_politik_lebensfern,
            einstellung_parteien_macht,
            n = 1,
            var = "pol_entfremdung_ix") %>% 
  row_sums(pol_part_wahl,
           pol_part_petition,
           pol_part_sm_kommentar,
           pol_part_partei_veranstaltung,
           pol_part_demo,
           pol_part_information,
           pol_part_gespraech,
           pol_part_produktboykott,
           pol_part_parteiengagement,
           pol_part_anderes_engagement,
           n = 5,
           var = "pol_part_sx") 
df <- df %>% 
  mutate(pol_entfremdung_mx = set_labels(pol_entfremdung_ix, labels = get_labels(df$einstellung_politiker_verstaendlich))) %>% 
  select(lfdn, pol_part_sx, pol_entfremdung_ix, alter) %>% 
  filter(!is.na(pol_part_sx), !is.na(pol_entfremdung_ix))
```

Dieser Abschnitt ist den Zusammenhängen zwischen metrischen Variablen gewidmet. Dabei wird zunächst auf die grafische Analyse eingegangen und dann die Berechnung der Kovarianz und des Korrelationskoeffizienten *r* veranschaulicht. Dabei werden sowohl die Befehle aus base-R, als auch die entsprechenden Befehle aus dem Pakte `psych` verwendet. Zudem wird noch das Paket `corrr` vorgestellt, das zur explorativen, grafischen Analyse von Korrelationen dient.

Zunächst werden die entsprechenden Pakete geladen.

```{r}
library(tidyverse) # für Scatterplots und die Pipe
library(psych)     # für Korrelationen
library(corrr)     # für Korrelationsmatrizen
```

Als Datenbeispiel dient wieder der Generation-Z-Datensatz. Ich habe in diesem Datensatz zwei Indices gebildet, deren Zusammenhang wir hier untersuchen wollen. 

- Für die *Politische Partizipation* habe ich einen Summenindex gebildet. Er zählt, wie viele von zehn möglichen Aktivitäten der politischen Partizipation eine Person bereits ausgeführt hat (z.B. Wählen gehen, Petitionen unterschreiben, demonstrieren oder Konsumboykott).

- Für die *Politische Entfremdung* habe ich einen Mittelwertindex gebildet, der auf fünf Items beruht, welche jeweils auf einer 4er-Skala von 1 = *stimme überhaupt nicht zu* bis 4 = *stimme voll und ganz zu* gemessen wurden. (Hier drei Beispielitems:  *Politik hat mit meinem Leben nichts zu tun*,  *Entscheidungsprozesse in der Politik sind für mich meistens nicht nachvollziehbar* und *Den Parteien geht es nur um Macht*).

Außerdem enthält der Datensatz noch die Variablen `lfdn` für die Fallnummer und das `alter` der Befragten.

```{r}
head(df)
```

Im folgenden soll nun die folgende Hypothese getestet werden:

*H1: Zwischen politischer Partizipation und politischer Entfremdung besteht ein negativer Zusammenhang.*

Diese Alternativhypothese steht im Gegensatz zur folgenden Nullhypothese:

*H0: Es gibt keinen (oder sogar einen positiven) Zusammenhang zwischen politischer Partizipation und politischer Entfremdung.*

Die Nullhypothese müsste beibehalten werden, sofern wir bei der Berechnung der Korrelation einen Wert von *r* berechnen der größer oder gleich 0 ist **oder** wenn wir zwar ein negatives *r* berechnen, aber der p-Wert indiziert, dass dieses berechnete *r* sich nicht signifikant von Null unterschiedet. Andernfalls können wir davon ausgehen, dass in der Grundgesamtheit wohl eher die H1 zutrifft.


### Streudiagramm

Wir starten zunächst mit einem Streudiagram/Scatterplot und nutzen dazu das Paket `ggplot2` aus dem Tidyverse. Das Paket wird im nächsten Kapitel (ab Januar) noch ausführlicher erläutert werden. Die Funktion zum Anlegen eines Plots in ggplot2 ist `ggplot()`. Sie benötigt als erstes Argument den Datensatz und dann als zweites Argument eine Hilfsfunktion, die `aes()` heißt. Diese Funktion ist für die *Ästhetik*, also das Aussehen des Plots, verantwortlich. In unserem Fall sind das die beiden Variablen, welche wir auf der X- und der Y-Achse anordnen. 

Nach dem Anlegen des Plots müssen wir dem Plot noch ein *Geom* hinzufügen. Der Begriff steht für *geom*etrisches Objekt. Ein Geom ist im Prinzip eine Funktion für die Art der Grafik. Es beinhaltet z.B. statistische Transformationen die zur Darstellung der Grafik nötig sind und Default-Layout-Informationen. In unserem Fall möchten wir das Geom `geom_jitter` hinzufügen, also einen "zitternden" Scatterplot. Eine Übersicht über verschiedene Geome findet man [hier](https://rstudio.com/wp-content/uploads/2015/06/ggplot2-german.pdf). Das Geom wird mit dem Plot über ein ` +` verknüpft. Diese Pluszeichen muss zwingend am Ende der vorigen Zeile stehen. Über das Pluszeichen kann man dem Plot auch noch weitere Veränderungen hinzufügen. Dazu später mehr. 

Alternativ zum oben beschriebenen Vorgehen kann man auch die *aes()*-Funktion in die *geom_*-Funktion einbauen, das macht optisch keinen Unterschied.

Hier der Code für das zitternde Streudiagram:

```{r}
df %>% 
  ggplot(aes(x = pol_part_sx, y = pol_entfremdung_ix)) +
  geom_jitter() 
```

Betrachtet man den Output kann man die Beziehung zwischen den beiden Variablen schon erahnen. Es zwar keine klare Linie ersichtlich (das wäre auch sehr viel verlangt), aber man kann schon sehen, dass in der Tendenz hohe Werte von politischer Entfremdung mit niedrigen Werten von politischer Partizipation einhergehen und umgekehrt. Die Grafik spricht also für den vermuteten negativen Zusammenhang.


### Kovarianz 

Die Kovarianz ist die gemeinsame Variation der beiden Variablen, bzw. das Produkt der Abweichung beider Variablen von ihrem jeweiligen Mittelwert geteilt durch die Fallzahl. In R kann man die Kovarianz einfach über den Befehl `cov()` ausgeben lassen  (Teil des `stats`-Paketes, wird üblicherweise mit base R geladen). Die Funktion benötigt lediglich die beiden Variablen/Vektoren, deren Kovarianz ermittelt werden soll:

```{r}
cov(df$pol_part_sx, df$pol_entfremdung_ix)
```

Im Beispiel ist die Kovarianz also `r round(cov(df$pol_part_sx, df$pol_entfremdung_ix), 2)`. Das ist insofern gut, wel das Vorzeichen der Prognose aus der Hypothese entspricht. Allerdings können wir noch keine Aussage über die Stärke des Zusammenhangs machen, weil die Kovarianz ein unstandardisiertes Maß für die gemeinsame Variation der beiden Variablen ist. Sie berücksichtigt die Skalierung der Variablen nicht.


### Korrelation mit `base R`/`stats`

Der Korrelationskoeffizient *r* (auch Pearson´s r oder Produkt-Moment-Korrelation) berücksichtigt die Skalierung, weil er die Standardabweichungen der beiden Variablen mit einbezieht. Er  beschreibt die Beziehung zwischen zwei metrischen Variablen in einem Wertebereich von -1 über 0 bis +1. Der Wert +1 steht dabei für eine perfekt positive und -1 für eine perfekt negative Beziehung. 

Auch der Korrelationskoeffizient lässt sich leicht mit dem `stats`-Paket berechnen:

```{r}
cor(df$pol_part_sx, df$pol_entfremdung_ix)
```

Das Vorzeichen bleibt verglichen mit der Kovarianz selbstverständlich dasselbe. Die Höhe des Betrags wird jedoch in einen Bereich zwischen 0 und 1 "gepresst". Für unsere beiden Variablen ergibt sich eine mittlere Effektstärke von *r* = `r round(cor(df$pol_part_sx, df$pol_entfremdung_ix), 2)`. 

Mit einem Signifikanztest, bei dem ein p-Wert berechnet wird, kann man außerdem prüfen, ob ein Korrelationskoeffizient sich signifikant von Null unterschiedet (Inferenzstatistik). Die Funktion für den Signifikanztest lautet `cor.test()`. Neben den beiden Variablen, kann man der Funktion weitere Argumente mitgeben:

- Das Argument `use` bestimmt darüber wie mit fehlenden werten umgegangen werden soll. Es ist eigentlich nur dann relevant, wenn mehr als zwei Variablen korreliert werden sollen. Dann kann man darüber entscheiden, ob ein FAll für alle mögliche Korrelationen ausgeschlossen werden soll, wenn er bei einer Variable einen fehlenden Wert hat (listenweiser Fallausschluss) oder ob dieser Fall nur bei den Korrelationen ausgeschlossen werden soll, bei denen die Variable beteiligt ist (paarweiser Fallausschluss).

- Im Argument `alternative` kann man festlegen, um was für eine Alternativhypothese es sich handelt. Hiernach bestimmt sich, in welche *Richtung* der Signifikanztest durchgeführt werden soll und ob *einseitig* oder *zweiseitig* getestet werden soll. Man kann hier die Option `two.sided` für einen zweiseitigen Test festlegen, wenn man eine ungerichtete Hypothese aufgestellt hat. Für gerichtete Hypothesen stehen die Optionen `greater` (für positive Zusammenhänge) und `less` (für negative Zusammenhänge) zur Verfügung.

```{r}
cor.test(df$pol_part_sx, df$pol_entfremdung_ix, 
         use = "complete.obs",
         alternative = "less")
```

Das Ergebnis ist ein kurzer "Bericht" über den Signifikanztest. Angegeben sind z.B. der p-Wert, das Konfidenzintervall und noch einmal der Korrelationskoeffizient. Aus dem p-Wert, der im Beispiel einen sehr niedrigen Wert (kleiner als die geforderten .05) aufweist, können wir schließen, dass der Wert *r* = `r cor(df$pol_part_sx, df$pol_entfremdung_ix)` signifikant von Null abweicht, also mit einiger Wahrscheinlichkeit nicht zufällig zustande gekommen ist. Das spricht für unsere Hypothese und damit für die Existenz des vermuteten Zusammenhangs in der Grundgesamtheit. Wir können die Hypothese somit als durch die Daten bestätigt ansehen.


### Korrelation mit `psych`

Den Korrelationskoeffizient kann man in R auch mit vielen anderen Paketen ausrechnen. Beispielhaft soll hier noch der Code für die Korrelation mit dem `psych`-Paket veranschaulicht werden. Dieses Paket benutzen wir ja auch für viele andere statistische Verfahren und man kann `psych` mit der Pipe benutzen (Tidyverse-Schreibweise). Der Output für die Korrelation sieht leicht anders aus. 

Die Funktion für die Korrelation in `psych` lautet `corr()` (mit 2 r). Sie benötigt als erstes Argument den Datensatz mit außschließlich den Variablen, die korreliert werden sollen. Diese können direkt vor der Funktion mit einem `select()`- Befehl ausgewählt werden. Neben dem Datenobjekt kann man weitere Argumente angeben, z.B. über `use` den listen- oder paarweisen Fallausschluss und über `method` die Art der Korrelation. Neben dem standardmäßig eingestellten Wert `pearson` für den Korrelationskoeffizienten (Pearson´s r) gibt es nämlich noch weitere Maßzahlen für spezielle Daten (z.B. `spearman` für Rangdaten oder `kendall` für ordinale Daten).

```{r}
df %>%
  select(pol_part_sx, pol_entfremdung_ix) %>% 
  psych::corr.test(use="pairwise", method="pearson")
```
Der Output sieht leicht anders aus als der oben dargestellte aus dem `stats`-Paket. Er hat drei wichtige Bereiche:

- Eine Matrix für die Korrelationskoeffizienten. Hier wird die Korrelation jeder Variablen mit jeder anderen im Datensatz dargestellt. In unserem Fall sind das ja nur zwei. Aber mit der Funktion könnten sie auch 3 oder noch mehr Variablen miteinander korrelieren. -- Jeweils natürlich nur paarweise. In dieser Matrix ist jede Korrelation doppelt enthalten: Einmal über und einmal unter der mittleren Diagonalen. Das liegt daran, dass die Korrelation 2 Mal berechnet wird: Zunächst mit der ersten Variable an erster und der zweiten an zweiter Stelle. Danach wird die Position der Variablen getauscht. Für Pearson´s r macht es jedoch keinen Unterschied, welche Reihenfolge die Variablen haben. Deshalb steht dort zweimal die gleiche Zahl. In der Diagonalen finden Sie die Korrelation einer Variablen mit sich selbst. Sie ist logischerweise jeweils = 1, also ein perfekter positiver Zusammenhang.

- Der zweite Bereich gibt Aufschluss über die Sample-Größe. Er wird auch manchmal als Matrix dargestellt, nämlich dann, wenn die Fallzahl für die einzelnen Korrelationen unterschiedlich wäre. Das ist hier aber nicht der Fall.

- Der dritte wichtige Bereich beinhaltet die p-Werte der Korrelationen. Im Beispiel sind alle p-Werte ausgesprochen niedrig, deshalb wird hier "0" dargestellt. Das ist natürlich der Rundung geschuldet, denn selbstverständlich ist der p-Wert nie exakt "0", da es sich um eine Wahrscheinlichkeit handelt. Er nähert sich lediglich dem Wert Null an.

### Partialkorrelation

Bei der Partialkorrelation wird der Einfluss einer dritten Variable aus der Korrelation zwischen zwei Variablen herausgerechnet. Das geschieht über die Residuen (vgl. zukünftiges Kapitel zur Regression/SDA2). Im `psych`-Paket kann man die Partialkorrelation einfach berechnen. Zur besseren Übersichtlichkeit kann man  vorab im `select()`-Befehl die Variablen auf eine spezielle Weise gruppieren (das kann man aber auch weglassen, dann muss man sich aber merken, welche Variable die Einflussvariable war). Im Anschluss erfolgt die Partialkorrelation durch die Funktion `partial.r()` und dann durch die Funktion `corr.p()` der entsprechende Signifikanztest:

```{r}
df %>% 
  select(x = c(pol_part_sx, pol_entfremdung_ix), y = alter) %>% 
  psych::partial.r() %>% 
  psych::corr.p(n =1003)
```

```{r, echo = FALSE}
dfr <- df %>% 
  select(x = c(pol_part_sx, pol_entfremdung_ix), y = alter) %>% 
  psych::partial.r() %>% 
  psych::corr.p(n =1003)

```

Der Output sieht ähnlich aus wie zuvor, nur dass in den Zellen jetzt jeweils die Korrelation zwischen zwei Variablen dargestellt ist, bereinigt um die jeweils dritte. Für die uns interessierende Korrelation zwischen politischer Partizipation und politischer Entfremdung ist der Korrelationskoeffizient hier nur leicht gesunken. Er beträgt jetzt noch *r-partial* = `r round(dfr$r[2,1], 2)`. Der Einfluss des Alters auf unseren Zusammenhang war also vermutlich nicht besonders stark. Auch nach Kontrolle dieser Drittvariable hat unsere Alternativhypothese also Bestand.

Man kann sogar in der Korrelations-Matrix oben sehen, dass das Alter lediglich mit der Variable *politische Partizipation* einen Zusammenhang hat, aber kaum mit *politischer Entfremdung*. Vermutlich wird ein Teil der Varianz in der politischen Partizipation durch das Alter erklärt. Das diese Variablen ebenfalls kovariieren macht inhaltlich sogar Sinn: Wer älter ist, hatte bereits mehr Gelegenheit zur politischen Partizipation und einige Partizipationsmöglichkeiten kann man sogar erst mit einem gewissen Alter ausüben, wie beispielsweise das Wählen.


### Korrelationsmatrizen darstellen

Zum Abschluss dieses Teils möchte ich noch kurz darauf eingehen, dass man natürlich auch mehrere oder sogar viele Korrelationen in einer Matrix darstellen kann. R liefert sogar ganz schöne Grafiken die Zusammenhänge zwischen metrischen Variablen übersichtlich darstellen können. Ein Paket, welches dazu benutzt werden kann, ist `corrr`.

Ich greife im folgenden auf einen anderen Datensatz zu, nämlich auf den Datensatz `mtcars` aus dem Tidyverse. In dem Datensatz sind Statistiken über verschiedene Automodelle gesammelt, aber der Inhalt ist an dieser Stelle nicht so wichtig.

Die Funktion `correlate()` aus dem `corrr`-Paket liefert zunächst die Korrelationsmatrix der Daten. Signifikanztest liefert das Paket nicht, denn es ist eher für die explorative Vorgehensweise geeignet (= nicht inferenzstatistisch-Hypothesenprüfend).

```{r}
mtcars %>% 
  corrr::correlate() 
```

Mit der Funktion `rplot()` kann man die Matrix in eine Korrelations-Grafik überführen:

```{r}
mtcars %>% 
  corrr::correlate() %>%
  corrr::rplot()

```

Das Paket liefert außerdem weitere Funktionen, die dabei helfen, die Matrix und damit auch die Grafik schöner zu formetieren. Mit `rearrange()` kann man die Variablen in der Matrix nach der der Größe der Korrelation sortieren. Mit `shave` kann man die "doppelte" obere Hälfte des Plots abschneiden. 

```{r}
mtcars %>% 
  corrr::correlate() %>%
  corrr::rearrange() %>%
  corrr::shave() %>% 
  corrr::rplot()
```

Sehr schön übersichtlich. Welche Variablen hier wie zusammenhängen sieht man auf den ersten Blick!
