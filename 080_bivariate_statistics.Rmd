# Bivariate Statistik

In diesem Kapitel geht es um bivariate Verfahren, also die gemeinsame Variation von zwei Variablen. Im Detail behandeln wir hier die Kreuztabelle und Chi-Quadrat sowie die Korrelation.


## Kreuztabellen

```{r, echo = FALSE, warning=FALSE, message=FALSE, error=FALSE}
pacman::p_load(tidyverse, janitor)
# Einlesen der Daten
data <- haven::read_sav("data/ZA6738_v1-0-0_generation_z_renamed.sav") %>% 
  select(lfdn, geschlecht, alter, starts_with("pol_part_")) %>% 
  filter(geschlecht < 3) %>% 
  mutate(alter_g3 = case_when(alter < 18 ~ "14 bis 17 Jahre",
                              alter < 22 ~ "18 bis 21 Jahre",
                              alter < 25 ~ "22 bis 24 Jahre")) %>% 
  select(lfdn, alter_g3, starts_with("pol_part_"))

```

Mit Kreuztabellen/Kontingenztabellen kann man die Verteilung einer Variable unter Berücksichtigung einer anderen in den Blick nehmen. Damit die Tabelle übersichtlich bleibt, sollten beide Variablen eher wenige Ausprägungen haben, also eher nominales oder ordinales Datenniveau haben. 

Chi-Quadrat ist eine Maßzahl für die Differenz zwischen der Kontingenztabelle (=gemessene Werte) und der Indifferenztabelle (=die Tabelle die entstünde, wenn es keinen Zusammenhang zwischen den Variablen geben würde). Ist Chi-Quadrat = 0, besteht kein Zusammenhang zwischen den Variablen. Allerdings kann Chi-Quadrat abhängig von der Reihen- und Spaltenzahl, sowie der Fallzahl, unendlich hohe Werte annehmen. Chi-Quadrate für unterschiedliche Tabellen lassen sich deshalb schlecht vergleichen. Mit Cramer´s V liegt eine standardisierte Form von Chi-Quadrat vor, die zwischen 0 und 1 variiert. Über die Richtung von Zusammenhängen gibt aber auch Cramer´s V keine Auskunft. Dazu muss man in der Kreuztabelle nachsehen. Kreuztabellen und Chi-Quadrat-basierte Maßzahlen sind bei Hypothesentests immer dann das Mittel der Wahl, wenn die abhängige Variable nominales Datenniveau hat.

Im Folgenden verwende ich wieder den Geneartion-Z-Datensatz als Beispiel. Darin gibt es einige Variablen zur politischen Partizipation, z.B. ob man schon einmal an einer Wahl teilgenommen hat oder schon einmal eine Petition unterschrieben hat. Diese Variablen sind dichotom 0/1-codiert. Die "0" bedeutet dabei, dass ein:e Befragte:r die Partizipationsmöglichkeit noch nie wahrgenommen hat und "1" bedeutet, dass sie mindestens einmal wahrgenommen wurde.

Außerdem enthält der Datensatz noch die Variable "alter_g3", die ich in drei Gruppen eingeteilt habe ("14 bis 17 Jahre", "18 bis 21 Jahre" und "22 bis 24 Jahre").

```{r}
head(data)
```

Ziel des nachfolgenden Skriptes ist es zu eruieren, ob sich der Anteil derjenigen, die eine Partizipationsmöglichkeit wahrgenommen haben, zwischen den Altersgruppen unterscheidet. Die Vermutung (Hypothese), die darin steckt ist natürlich, dass bei zunehmendem Alter der Anteil derjenigen steigt, die diese Möglichkeit bereits wahrgenommen haben. Das Beispiel hier im Buch beschäftigt sich insbesondere mit dem Unterschreiben von Petitionen. 

Unsere H1 lautet also:

*Der Anteil derjenigen, die bereits eine Petition unterschrieben haben, steigt mit zunehmendem Alter.*

Bevor es mit dem Hypothesentest losgehen kann, müssen die erforderlichen Pakete geladen werden. Das tidyverse für die Pipe, `janitor` für die Kreuztabellen und Chi-Quadrat (χ<sup>2</sup>) und `DescTools` für Cramer´s V.

```{r}
library(tidyverse)
library(janitor)
library(DescTools)
```


### Vorbereitung: Univariate Verteilung

Schauen wir uns zunächst einmal die univariate Verteilung der beiden Variablen an.
Dies ist hilfreich, um ein Gefühl für die Daten zu bekommen und ein Verständnis dafür zu entwickeln, welche Verteilung wir erwarten würden.
Das geht (wie im Kapitel zu den Häufigkeitstabellen beschrieben) am schönsten mit dem Paket `janitor` und der Funktion `tabyl()`.

```{r}
# Häufigkeitstabelle Altersgruppen
tabyl(data$alter_g3)
```

Die drei Altersgruppen sind also alle etwa gleich stark besetzt, die älteste Altersgruppe ist ca. 5 Prozent kleiner als die anderen beiden.

Jetzt noch die Beteiligung an Petitionen:

```{r}
# Häufigkeitstabelle Teilnahme Petitionen
tabyl(data$pol_part_petition)
```

Ein knappes Drittel der Befragten haben bereits eine Petition unterschrieben. Würde kein Zusammenhang/Unterschied in den Gruppen vorliegen, wäre also zu erwarten, dass etwa ein Drittel der Befragten in jeder Altersgruppe bereits eine Petition unterschrieben hat.

### Kreuztabelle ausgeben

Mit dem Paket `janitor` und der Funktion `tabyl()` kann man nicht nur einfache Tabellen erstellen, sondern auch Kreuztabellen. Dazu gibt man die beiden Variablen, die man kreuztabulieren möchte, einfach nacheinander als Argumente in die Funktion. Die Variable, die zuerst übergeben wird, steht dann hinterher in den Zeilen, die zweite in den Spalten. Es ist eine Konvention, dass Variablen, die als unabhängig betrachtet werden, bei Kreuztabellen in den Spalten dargestellt werden. An einigen Stellen findet man es aber auch andersherum. Das Layout einer Tabelle hängt ja auch manchmal davon ab, wo man wieviel Platz hat und wenn man eine unabhängige Variable mit sehr vielen Ausprägungen hat, dann passt sie unter Umständen besser in die Zeilen.

Wir halten uns im folgenden Code jedoch an die Konvention und übergeben zusätzlich noch das Argument `show_na = FALSE` um fehlende Werte aus der Analyse auszuschließen.

Hier der Basis-Code für die Kreuztabelle mit `janitor::tabyl()`:

```{r}
# Kreuztabelle berechnen
my_crosstab <- data %>%
  janitor::tabyl(pol_part_petition, alter_g3, show_na = FALSE) 

my_crosstab
```

Die Tabelle macht genau was sie soll, sie tabuliert die beiden Variablen im vorgegebenen Layout und gibt dabei die absoluten Häufigkeiten aus. Jetzt wäre es natürlich schön, wenn wir die Tabelle weiter formatieren können und z.B. Prozentwerte und auch Randspalten hinzufügen könnten. Das geht natürlich auch. Dazu beinhaltet das `janitor`-Paket eine Reihe von Funktionen, die alle mit `adorn_` beginnen, z.B.:

- `adorn_totals()` fügt Randhäufigkeiten hinzu. Mit dem Argument `where = ` kann man noch bestimmen, ob dies in den Spalten (`"col"`), oder in den Reihen (`"row"`) oder in beidem `c("row", "col")` geschehen soll.

- `adorn_percentages()` berechnet die Prozentwerte. Mit dem Argument `denominator = ` kann man noch bestimmen, ob dies in den Spalten (`"col"`), oder in den Reihen (`"row"`) oder in beidem `"all"` geschehen soll.

- `adorn_pct_formatting()` dient der Formatierung der Prozentwerte. Über das Argument `digits =`  kann man die Anzahl der Nachkommastellen festlegen.

- `adorn_ns()` fügt die absoluten Häufigkeiten wieder hinzu. Denn diese werden bei der Formatierung in Prozentwerte durch `adorn_percentages()` überschreiben.

- `adorn_title()` dient zur Beschriftung der Tabelle. Mit `placement = "combined"` kann man z.B. in der ersten Zelle die beiden Variablennamen anzeigen lassen. Eine andere Variante wäre `placement = "top"`.

Probieren wir es aus:

```{r}
# Kreuztabelle formatieren
my_crosstab %>% 
  adorn_totals(where = c("row", "col")) %>%
  adorn_percentages(denominator = "col") %>% 
  adorn_pct_formatting(digits = 0) %>%
  adorn_ns() %>%
  adorn_title(placement = "combined")

```

Sehr hübsch! Durch die übersichtliche Formatierung mit den Prozentwerten können wir jetzt gut vergleichen, wie sich der Anteil derjenigen, die bereits Petitionen unterschreiben haben in den Altersgruppen unterscheidet. Zur Erinnerung, im Gesamten Sample waren es 32 Prozent, die diese Form der politischen Partizipation bereits genutzt haben (siehe auch Spalte "Total").

Vergleicht man nun die Altersgruppen sieht man deutliche Unterschiede:

- Insbesondere die erste Gruppe der 14- bis 17-Jährigen hat deutlich weniger Petitionen unterschrieben, als die anderen beiden Gruppen. Dies war erwartbar und entspricht im auch der Hypothese, die wir eingangs formuliert hatten. Möglicherweise spielt für diese Art der politischen Partizipation die Volljährigkeit eine besondere Rolle? 

- Zwischen den älteren beiden Altersgruppen ist hingegen kaum ein Unterschied. Der Prozentsatz sinkt sogar leicht ab, was unserer Hypothese nicht entsprechen würde. Allerdings ist die Differenz ohnehin sehr gering und kaum von Bedeutung.

Nach dem Augenschein der Kreuztabelle, scheinen wir also insgesamt auf einen interessanten Zusammenhang gestoßen zu sein, der unserer Hypothese auch entspricht. Aber ist dieser Zusammenhang auch signifikant?

### Chi-Quadrat & Cramer´s V

Dazu ziehen wir im folgenden den Chi-Quadrat-Test heran, ebenfalls aus dem Paket `janitor`.

```{r}
# Chi-Quadrat berchnen
janitor::chisq.test(my_crosstab)
```

Chi-Quadrat beträgt 37.2 (df = 2), bei einem sehr kleinen p-Wert. Der p-Wert 8.249e-09 bedeutet 8.249 * 10 ^ -9 also 0.000000008249. Das ist deutlich unter p < .001 und damit "signifikant". Wir können deshalb davon ausgehen, dass der Zusammenhang/Unterschied, den wir hier beobachtet haben, überzufällig zu Stande gekommen ist. Die Daten unterstützen also unsere Hypothese H1.

Aber wie stark ist der gefundene Zusammenhang? Dabei hilft uns Cramer´s V, quasi das standardisierte Chi-Quadrat. Die Funktion dazu findet sich im Paket `DescTools` und heißt `CramerV()`. Sie benötigt als einziges Argument eine Kreuztabelle, bzw. die darin befindlichen Zahlen als Matrix (also auf keinen Fall die formatierte Tabelle). Die einfache Tabelle haben wir oben im Objekt `my_crosstab` gespeichert. Für die Berechnung von Cramer´s V muss noch die erste Spalte gelöscht werden, die die Ausprägungen der Variable zu Petitionen enthält. Über das Subsetting `[, -1]` können wir genau dies erreichen. Der Befehl löscht quasi alle Zeilen (durch das Weglassen der Angabe vor dem Komma) und alle Spalten bis auf die erste (nach dem Komma `-1`).

```{r}
# Cramer´s V
DescTools::CramerV(my_crosstab[, -1])
```

Cramer´s V beträgt .19. Es besteht also ein schwacher, aber signifikanter Zusammenhang zwischen dem Alter und der politischen Beteiligung mittels Petitionen. 

Die Hypothese kann damit insgesamt als bestätigt angesehen werden, auch wenn wir einräumen müssen, dass nicht zwischen allen Altersgruppen Unterschiede bestehen.
Stattdessen wird offenbar durch das Erreichen der Volljährigkeit ein relevanter Anstieg beim Unterzeichnen von Petitionen befördert. Spannend!


## Korrelationen

```{r, echo = FALSE, warning=FALSE, message=FALSE, error=FALSE}
pacman::p_load(tidyverse, sjmisc)
# Einlesen der Daten
df <- haven::read_sav("data/ZA6738_v1-0-0_generation_z_recoded.sav") %>% 
  row_means(einstellung_politiker_verstaendlich,
            einstellung_entscheidungsprozess_undurchsichtig,
            einstellung_keine_ueberzeugende_partei,
            einstellung_politik_lebensfern,
            einstellung_parteien_macht,
            n = 1,
            var = "pol_entfremdung_ix") %>% 
  row_sums(pol_part_wahl,
           pol_part_petition,
           pol_part_sm_kommentar,
           pol_part_partei_veranstaltung,
           pol_part_demo,
           pol_part_information,
           pol_part_gespraech,
           pol_part_produktboykott,
           pol_part_parteiengagement,
           pol_part_anderes_engagement,
           n = 5,
           var = "pol_part_sx") 
df <- df %>% 
  mutate(pol_entfremdung_mx = set_labels(pol_entfremdung_ix, labels = get_labels(df$einstellung_politiker_verstaendlich))) %>% 
  select(lfdn, pol_part_sx, pol_entfremdung_ix, alter) %>% 
  filter(!is.na(pol_part_sx), !is.na(pol_entfremdung_ix))
```

Dieser Abschnitt ist den Zusammenhängen zwischen metrischen Variablen gewidmet. Dabei wird zunächst auf die grafische Analyse eingegangen und dann die Berechnung der Kovarianz und des Korrelationskoeffizienten *r* veranschaulicht. Dabei werden sowohl die Befehle aus base-R, als auch die entsprechenden Befehle aus dem Pakte `psych` verwendet. Zudem wird noch das Paket `corrr` vorgestellt, das zur explorativen, grafischen Analyse von Korrelationen dient.

Zunächst werden die entsprechenden Pakete geladen.

```{r}
library(tidyverse) # für Scatterplots und die Pipe
library(psych)     # für Korrelationen
library(corrr)     # für Korrelationsmatrizen
```

Als Datenbeispiel dient wieder der Generation-Z-Datensatz. Ich habe in diesem Datensatz zwei Indices gebildet, deren Zusammenhang wir hier untersuchen wollen. 

- Für die *Politische Partizipation* habe ich einen Summenindex gebildet. Er zählt, wie viele von zehn möglichen Aktivitäten der politischen Partizipation eine Person bereits ausgeführt hat (z.B. Wählen gehen, Petitionen unterschreiben, demonstrieren oder Konsumboykott).

- Für die *Politische Entfremdung* habe ich einen Mittelwertindex gebildet, der auf fünf Items beruht, welche jeweils auf einer 4er-Skala von 1 = *stimme überhaupt nicht zu* bis 4 = *stimme voll und ganz zu* gemessen wurden. (Hier drei Beispielitems:  *Politik hat mit meinem Leben nichts zu tun*,  *Entscheidungsprozesse in der Politik sind für mich meistens nicht nachvollziehbar* und *Den Parteien geht es nur um Macht*).

Außerdem enthält der Datensatz noch die Variablen `lfdn` für die Fallnummer und das `alter` der Befragten.

```{r}
head(df)
```

Im folgenden soll nun die folgende Hypothese getestet werden:

*H1: Zwischen politischer Partizipation und politischer Entfremdung besteht ein negativer Zusammenhang.*

Diese Alternativhypothese steht im Gegensatz zur folgenden Nullhypothese:

*H0: Es gibt keinen (oder sogar einen positiven) Zusammenhang zwischen politischer Partizipation und politischer Entfremdung.*

Die Nullhypothese müsste beibehalten werden, sofern wir bei der Berechnung der Korrelation einen Wert von *r* berechnen der größer oder gleich 0 ist **oder** wenn wir zwar ein negatives *r* berechnen, aber der p-Wert indiziert, dass dieses berechnete *r* sich nicht signifikant von Null unterschiedet. Andernfalls können wir davon ausgehen, dass in der Grundgesamtheit wohl eher die H1 zutrifft.


### Streudiagramm

Wir starten zunächst mit einem Streudiagram/Scatterplot und nutzen dazu das Paket `ggplot2` aus dem Tidyverse. Das Paket wird im nächsten Kapitel (ab Januar) noch ausführlicher erläutert werden. Die Funktion zum Anlegen eines Plots in ggplot2 ist `ggplot()`. Sie benötigt als erstes Argument den Datensatz und dann als zweites Argument eine Hilfsfunktion, die `aes()` heißt. Diese Funktion ist für die *Ästhetik*, also das Aussehen des Plots, verantwortlich. In unserem Fall sind das die beiden Variablen, welche wir auf der X- und der Y-Achse anordnen. 

Nach dem Anlegen des Plots müssen wir dem Plot noch ein *Geom* hinzufügen. Der Begriff steht für *geom*etrisches Objekt. Ein Geom ist im Prinzip eine Funktion für die Art der Grafik. Es beinhaltet z.B. statistische Transformationen die zur Darstellung der Grafik nötig sind und Default-Layout-Informationen. In unserem Fall möchten wir das Geom `geom_jitter` hinzufügen, also einen "zitternden" Scatterplot. Eine Übersicht über verschiedene Geome findet man [hier](https://rstudio.com/wp-content/uploads/2015/06/ggplot2-german.pdf). Das Geom wird mit dem Plot über ein ` +` verknüpft. Diese Pluszeichen muss zwingend am Ende der vorigen Zeile stehen. Über das Pluszeichen kann man dem Plot auch noch weitere Veränderungen hinzufügen. Dazu später mehr. 

Alternativ zum oben beschriebenen Vorgehen kann man auch die *aes()*-Funktion in die *geom_*-Funktion einbauen, das macht optisch keinen Unterschied.

Hier der Code für das zitternde Streudiagram:

```{r}
df %>% 
  ggplot(aes(x = pol_part_sx, y = pol_entfremdung_ix)) +
  geom_jitter() 
```

Betrachtet man den Output kann man die Beziehung zwischen den beiden Variablen schon erahnen. Es zwar keine klare Linie ersichtlich (das wäre auch sehr viel verlangt), aber man kann schon sehen, dass in der Tendenz hohe Werte von politischer Entfremdung mit niedrigen Werten von politischer Partizipation einhergehen und umgekehrt. Die Grafik spricht also für den vermuteten negativen Zusammenhang.


### Kovarianz 

Die Kovarianz ist die gemeinsame Variation der beiden Variablen, bzw. das Produkt der Abweichung beider Variablen von ihrem jeweiligen Mittelwert geteilt durch die Fallzahl. In R kann man die Kovarianz einfach über den Befehl `cov()` ausgeben lassen  (Teil des `stats`-Paketes, wird üblicherweise mit base R geladen). Die Funktion benötigt lediglich die beiden Variablen/Vektoren, deren Kovarianz ermittelt werden soll:

```{r}
cov(df$pol_part_sx, df$pol_entfremdung_ix)
```

Im Beispiel ist die Kovarianz also `r round(cov(df$pol_part_sx, df$pol_entfremdung_ix), 2)`. Das ist insofern gut, wel das Vorzeichen der Prognose aus der Hypothese entspricht. Allerdings können wir noch keine Aussage über die Stärke des Zusammenhangs machen, weil die Kovarianz ein unstandardisiertes Maß für die gemeinsame Variation der beiden Variablen ist. Sie berücksichtigt die Skalierung der Variablen nicht.


### Korrelation mit `base R`/`stats`

Der Korrelationskoeffizient *r* (auch Pearson´s r oder Produkt-Moment-Korrelation) berücksichtigt die Skalierung, weil er die Standardabweichungen der beiden Variablen mit einbezieht. Er  beschreibt die Beziehung zwischen zwei metrischen Variablen in einem Wertebereich von -1 über 0 bis +1. Der Wert +1 steht dabei für eine perfekt positive und -1 für eine perfekt negative Beziehung. 

Auch der Korrelationskoeffizient lässt sich leicht mit dem `stats`-Paket berechnen:

```{r}
cor(df$pol_part_sx, df$pol_entfremdung_ix)
```

Das Vorzeichen bleibt verglichen mit der Kovarianz selbstverständlich dasselbe. Die Höhe des Betrags wird jedoch in einen Bereich zwischen 0 und 1 "gepresst". Für unsere beiden Variablen ergibt sich eine mittlere Effektstärke von *r* = `r round(cor(df$pol_part_sx, df$pol_entfremdung_ix), 2)`. 

Mit einem Signifikanztest, bei dem ein p-Wert berechnet wird, kann man außerdem prüfen, ob ein Korrelationskoeffizient sich signifikant von Null unterschiedet (Inferenzstatistik). Die Funktion für den Signifikanztest lautet `cor.test()`. Neben den beiden Variablen, kann man der Funktion weitere Argumente mitgeben:

- Das Argument `use` bestimmt darüber wie mit fehlenden werten umgegangen werden soll. Es ist eigentlich nur dann relevant, wenn mehr als zwei Variablen korreliert werden sollen. Dann kann man darüber entscheiden, ob ein FAll für alle mögliche Korrelationen ausgeschlossen werden soll, wenn er bei einer Variable einen fehlenden Wert hat (listenweiser Fallausschluss) oder ob dieser Fall nur bei den Korrelationen ausgeschlossen werden soll, bei denen die Variable beteiligt ist (paarweiser Fallausschluss).

- Im Argument `alternative` kann man festlegen, um was für eine Alternativhypothese es sich handelt. Hiernach bestimmt sich, in welche *Richtung* der Signifikanztest durchgeführt werden soll und ob *einseitig* oder *zweiseitig* getestet werden soll. Man kann hier die Option `two.sided` für einen zweiseitigen Test festlegen, wenn man eine ungerichtete Hypothese aufgestellt hat. Für gerichtete Hypothesen stehen die Optionen `greater` (für positive Zusammenhänge) und `less` (für negative Zusammenhänge) zur Verfügung.

```{r}
cor.test(df$pol_part_sx, df$pol_entfremdung_ix, 
         use = "complete.obs",
         alternative = "less")
```

Das Ergebnis ist ein kurzer "Bericht" über den Signifikanztest. Angegeben sind z.B. der p-Wert, das Konfidenzintervall und noch einmal der Korrelationskoeffizient. Aus dem p-Wert, der im Beispiel einen sehr niedrigen Wert (kleiner als die geforderten .05) aufweist, können wir schließen, dass der Wert *r* = `r cor(df$pol_part_sx, df$pol_entfremdung_ix)` signifikant von Null abweicht, also mit einiger Wahrscheinlichkeit nicht zufällig zustande gekommen ist. Das spricht für unsere Hypothese und damit für die Existenz des vermuteten Zusammenhangs in der Grundgesamtheit. Wir können die Hypothese somit als durch die Daten bestätigt ansehen.


### Korrelation mit `psych`

Den Korrelationskoeffizient kann man in R auch mit vielen anderen Paketen ausrechnen. Beispielhaft soll hier noch der Code für die Korrelation mit dem `psych`-Paket veranschaulicht werden. Dieses Paket benutzen wir ja auch für viele andere statistische Verfahren und man kann `psych` mit der Pipe benutzen (Tidyverse-Schreibweise). Der Output für die Korrelation sieht leicht anders aus. 

Die Funktion für die Korrelation in `psych` lautet `corr()` (mit 2 r). Sie benötigt als erstes Argument den Datensatz mit außschließlich den Variablen, die korreliert werden sollen. Diese können direkt vor der Funktion mit einem `select()`- Befehl ausgewählt werden. Neben dem Datenobjekt kann man weitere Argumente angeben, z.B. über `use` den listen- oder paarweisen Fallausschluss und über `method` die Art der Korrelation. Neben dem standardmäßig eingestellten Wert `pearson` für den Korrelationskoeffizienten (Pearson´s r) gibt es nämlich noch weitere Maßzahlen für spezielle Daten (z.B. `spearman` für Rangdaten oder `kendall` für ordinale Daten).

```{r}
df %>%
  select(pol_part_sx, pol_entfremdung_ix) %>% 
  psych::corr.test(use="pairwise", method="pearson")
```
Der Output sieht leicht anders aus als der oben dargestellte aus dem `stats`-Paket. Er hat drei wichtige Bereiche:

- Eine Matrix für die Korrelationskoeffizienten. Hier wird die Korrelation jeder Variablen mit jeder anderen im Datensatz dargestellt. In unserem Fall sind das ja nur zwei. Aber mit der Funktion könnten sie auch 3 oder noch mehr Variablen miteinander korrelieren. -- Jeweils natürlich nur paarweise. In dieser Matrix ist jede Korrelation doppelt enthalten: Einmal über und einmal unter der mittleren Diagonalen. Das liegt daran, dass die Korrelation 2 Mal berechnet wird: Zunächst mit der ersten Variable an erster und der zweiten an zweiter Stelle. Danach wird die Position der Variablen getauscht. Für Pearson´s r macht es jedoch keinen Unterschied, welche Reihenfolge die Variablen haben. Deshalb steht dort zweimal die gleiche Zahl. In der Diagonalen finden Sie die Korrelation einer Variablen mit sich selbst. Sie ist logischerweise jeweils = 1, also ein perfekter positiver Zusammenhang.

- Der zweite Bereich gibt Aufschluss über die Sample-Größe. Er wird auch manchmal als Matrix dargestellt, nämlich dann, wenn die Fallzahl für die einzelnen Korrelationen unterschiedlich wäre. Das ist hier aber nicht der Fall.

- Der dritte wichtige Bereich beinhaltet die p-Werte der Korrelationen. Im Beispiel sind alle p-Werte ausgesprochen niedrig, deshalb wird hier "0" dargestellt. Das ist natürlich der Rundung geschuldet, denn selbstverständlich ist der p-Wert nie exakt "0", da es sich um eine Wahrscheinlichkeit handelt. Er nähert sich lediglich dem Wert Null an.

### Partialkorrelation

Bei der Partialkorrelation wird der Einfluss einer dritten Variable aus der Korrelation zwischen zwei Variablen herausgerechnet. Das geschieht über die Residuen (vgl. zukünftiges Kapitel zur Regression/SDA2). Im `psych`-Paket kann man die Partialkorrelation einfach berechnen. Zur besseren Übersichtlichkeit kann man  vorab im `select()`-Befehl die Variablen auf eine spezielle Weise gruppieren (das kann man aber auch weglassen, dann muss man sich aber merken, welche Variable die Einflussvariable war). Im Anschluss erfolgt die Partialkorrelation durch die Funktion `partial.r()` und dann durch die Funktion `corr.p()` der entsprechende Signifikanztest:

```{r}
df %>% 
  select(x = c(pol_part_sx, pol_entfremdung_ix), y = alter) %>% 
  psych::partial.r() %>% 
  psych::corr.p(n =1003)
```

```{r, echo = FALSE}
dfr <- df %>% 
  select(x = c(pol_part_sx, pol_entfremdung_ix), y = alter) %>% 
  psych::partial.r() %>% 
  psych::corr.p(n =1003)

```

Der Output sieht ähnlich aus wie zuvor, nur dass in den Zellen jetzt jeweils die Korrelation zwischen zwei Variablen dargestellt ist, bereinigt um die jeweils dritte. Für die uns interessierende Korrelation zwischen politischer Partizipation und politischer Entfremdung ist der Korrelationskoeffizient hier nur leicht gesunken. Er beträgt jetzt noch *r-partial* = `r round(dfr$r[2,1], 2)`. Der Einfluss des Alters auf unseren Zusammenhang war also vermutlich nicht besonders stark. Auch nach Kontrolle dieser Drittvariable hat unsere Alternativhypothese also Bestand.

Man kann sogar in der Korrelations-Matrix oben sehen, dass das Alter lediglich mit der Variable *politische Partizipation* einen Zusammenhang hat, aber kaum mit *politischer Entfremdung*. Vermutlich wird ein Teil der Varianz in der politischen Partizipation durch das Alter erklärt. Das diese Variablen ebenfalls kovariieren macht inhaltlich sogar Sinn: Wer älter ist, hatte bereits mehr Gelegenheit zur politischen Partizipation und einige Partizipationsmöglichkeiten kann man sogar erst mit einem gewissen Alter ausüben, wie beispielsweise das Wählen.


### Korrelationsmatrizen darstellen

Zum Abschluss dieses Teils möchte ich noch kurz darauf eingehen, dass man natürlich auch mehrere oder sogar viele Korrelationen in einer Matrix darstellen kann. R liefert sogar ganz schöne Grafiken die Zusammenhänge zwischen metrischen Variablen übersichtlich darstellen können. Ein Paket, welches dazu benutzt werden kann, ist `corrr`.

Ich greife im folgenden auf einen anderen Datensatz zu, nämlich auf den Datensatz `mtcars` aus dem Tidyverse. In dem Datensatz sind Statistiken über verschiedene Automodelle gesammelt, aber der Inhalt ist an dieser Stelle nicht so wichtig.

Die Funktion `correlate()` aus dem `corrr`-Paket liefert zunächst die Korrelationsmatrix der Daten. Signifikanztest liefert das Paket nicht, denn es ist eher für die explorative Vorgehensweise geeignet (= nicht inferenzstatistisch-Hypothesenprüfend).

```{r}
mtcars %>% 
  corrr::correlate() 
```

Mit der Funktion `rplot()` kann man die Matrix in eine Korrelations-Grafik überführen:

```{r}
mtcars %>% 
  corrr::correlate() %>%
  corrr::rplot()

```

Das Paket liefert außerdem weitere Funktionen, die dabei helfen, die Matrix und damit auch die Grafik schöner zu formetieren. Mit `rearrange()` kann man die Variablen in der Matrix nach der der Größe der Korrelation sortieren. Mit `shave` kann man die "doppelte" obere Hälfte des Plots abschneiden. 

```{r}
mtcars %>% 
  corrr::correlate() %>%
  corrr::rearrange() %>%
  corrr::shave() %>% 
  corrr::rplot()
```

Sehr schön übersichtlich. Welche Variablen hier wie zusammenhängen sieht man auf den ersten Blick!


## Wichtige Funktionen aus diesem Kapitel {-}

| Funktion           | Paket        | Beschreibung                    | Wichtige Argumente/Bemerkung |
|--------------------|--------------|---------------------------------|-----------------------|
| **Tabellenanalyse**                                                                         |
|`tabyl()`           | janitor      | Tabellen & Kreuztabellen        | `show_na = FALSE`     |
|`adorn_totals()`    | janitor      | Randhäufigkeiten hinzufügen     | `where = c("row", "col")` |
|`adorn_percentages` | janitor      | In Prozentwerte umwandeln       | `denominator = "col"` |
|`adorn_pct_formatting`| janitor    | Formatierung der Prozentwerte   | `digits = n`          |
|`adorn_ns()`        | janitor      | Absolute Häufigkeiten wieder hinzufügen |               |
|`adorn_titel()`     | janitor      | Variablen in die erste Zelle schreiben  | `placement = "combined"` |
|`chisq.test()`      | janitor      | Chi-Quadrat-Test                | einfache Kreuztabelle |
|`CramerV()`         | DescTools    | Cramer´s V                      | Kreuztabelle ohne erste Spalte! |
| **Kovarianz & Korrelation**                                                   |
|`cov()`             | stats        | Kovarianz                       |                       |
|`cor()`             | stats        | Korrelation                     |                       |
|`cor.test()`        | stats        | Signifikanztest für r           | `use`, `alternative`  |
|`corr.test()`       | psych        | Korrelation + Signifikanztest   | `use`, `method`       |
|`partial.r()`       | psych        | Partialkorrelation              |                       |
|`corr.p()`          | psych        | Signifikanztest für Partialkorrelation   | `n`                   |
|`correlate()`       | corrr        | Korrelationsmatrix              |                       |
| **Grafiken**                                                                                |
|`ggplot()`          | ggplot2      | Plot anlegen                    | `aes()`               |
|`geom_jitter()`     | ggplot2      | "zitternder" Scatterplot        |                       |
|`aes()`             | ggplot2      | "Ästhetik" des Plots            | Variablen, die im Plot darzustellen sind |
|`rplot()`           | corrr        | Korrelations-Plot               |                       |
